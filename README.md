# ğŸ“Š LLMOps Dashboard

![CI](https://github.com/Cre4T3Tiv3/llmops-dashboard/actions/workflows/ci.yml/badge.svg?branch=main)
![License](https://img.shields.io/github/license/Cre4T3Tiv3/llmops-dashboard)
![Stars](https://img.shields.io/github/stars/Cre4T3Tiv3/llmops-dashboard?style=social)

## âœ¨ Why This Exists

LLMOps Dashboard is a modular open-source observability stack
for LLM systems â€” built with FastAPI, Prometheus, Grafana, and SQLite.

ğŸ’¡ Powered by **LLaMA 3 (via Ollama)** for secure, local inference.

It helps you monitor:

* ğŸ§  Prompt/response metadata
* â±ï¸ Latency (p95, per-user)
* ğŸ“‰ Token usage and fallback behavior
* ğŸ” JWT-based user tracking
* ğŸ“Š Real-time dashboards for analysis

This OSS project provides a full-stack **starter template** for building
**production-grade observability for LLM applications** â€” local or cloud.

> âœ… Built for local-first development, extensibility, and minimal infra overhead.

---

## ğŸ§  Model Control Plane (MCP)

The MCP (Model Control Plane) is a lightweight module that tracks **which models are used**, by **whom**, and under **what policy constraints**.

It enables:

| Capability                | Description                                                               |
| ------------------------- | ------------------------------------------------------------------------- |
| ğŸ”¢ Model Registration     | Track models by name, size, alias, and source                             |
| ğŸ‘¤ Per-Client Policies    | Enforce token limits per user/client                                      |
| ğŸ”€ Dynamic Policy Control | Policies can be modified at runtime or pre-configured at boot             |
| ğŸ“Š Metrics Integration    | Token counts and usage policies propagate into `/metrics` Prometheus feed |
| ğŸªª Identity Tracking      | Associates JWT-authenticated users with tracked model usage               |

Example usage:

```python
from llmops.mcp import model_registry, usage_policy

# Register a model
model_registry.register_model("llama3", "8b", alias="dev")

# Apply a per-user token limit
usage_policy.set_policy("client-x", max_tokens=5000)
```

> ğŸ§  This system can evolve into a **policy enforcement and audit framework**, especially in multi-user environments where tracking LLM usage, enforcing limits, or billing per token becomes critical.

---

## ğŸ”§ What It Does (Current Stack)

| Feature                 | Description                                             |
| ----------------------- | ------------------------------------------------------- |
| ğŸ” JWT Auth             | Secure `/llm` with per-user access tokens               |
| ğŸ§  MCP Integration      | Tracks model usage, policy limits, token stats          |
| ğŸ“ˆ Prometheus Metrics   | Request rate, p95 latency, fallback %, etc.             |
| ğŸ“Š Grafana Dashboard    | Includes working starter panels for request and latency |
| ğŸ“‚ SQLite Audit Trail   | Logs prompt, user, model, token count                   |
| ğŸ§ª Simulation + Testing | Run `make simulate` or `make smoke-test`                |
| ğŸ¤– LLM Integration     | Supports mock + real LLaMA 3 (Ollama) model endpoints    |
| ğŸ¦™ LLaMA 3 (Ollama)    | Real local inference via `/llm/echo` using Ollama        |

## ğŸ¤– LLM Integration Status

This project supports **both mock and real LLM inference**:

### âœ… `/llm` (Simulated)

Default route for testing:

- Returns mock responses instantly
- Used for simulating traffic and testing fallback logic
- No network or real model required

```python
# Simulate fallback model logic
model_used = "openai-gpt"
if random.random() < 0.3:
    model_used = "local-ollama"
return {"response": f"[{model_used.capitalize()}] Answer to: {prompt}"}
````

### âœ… `/llm/echo` (Real LLaMA 3 via Ollama)

Backed by real local inference using [Ollama](https://ollama.com):

```bash
ollama run llama3
```

Once pulled, the model runs **offline** and is used for actual inference.

To call:

```bash
curl -X POST http://localhost:8000/llm/echo \
 -H "Authorization: Bearer <your-jwt>" \
 -H "x-user-id: demo-user" \
 -H "Content-Type: application/json" \
 -d '{"prompt": "What is vector search?"}'
```
---

## ğŸ”­ Planned Integrations (Roadmap)

âœ… Currently supported:

* âœ… Local LLM echo endpoint via `llama3` + Ollama (`/llm/echo`)
* âœ… GPU-ready Docker support with offline model warmup
* âœ… Prometheus + Grafana instrumentation
* âœ… Secure JWT-authenticated observability for LLM events
* âœ… SQLite-based request logging
* âœ… Test coverage and E2E support

ğŸš§ Coming soon:

* [ ] Auto Summary Mode
  - Nightly background task summarizes recent logs via LLM
  - Stored in DB or JSON for display in Grafana summary panel

* [ ] Copilot UI Widget
  - Frontend prompt box sends input to `/llm`
  - Response is streamed or displayed with built-in observability

* [ ] Runtime LLM backend toggle (OpenAI, Ollama, HF)
* [ ] OAuth / Auth0 provider support
* [ ] Token pricing and billing estimation
* [ ] Slack alerting or LLM log summaries

ğŸ“¦ Pluggable LLM Providers:

* [ ] OpenAI API via `openai.ChatCompletion`
* [x] Local Ollama models via `ollama run`
* [ ] Hugging Face `transformers` with local inference engine

> ğŸ’¡ Contributions welcome â€” especially around modular LLM adapters and frontend UX.

---

## ğŸ” JWT Secrets

This project **requires** `JWT_SECRET` to be set via `.env`, environment variables, or secret injection.

```env
JWT_SECRET=supersecretkey        # âš ï¸ For local testing only (ChangeMe)
```

### ğŸ” Used in code

```python
# token_issuer.py / auth.py
JWT_SECRET = os.getenv("JWT_SECRET")
if not JWT_SECRET:
    raise RuntimeError("JWT_SECRET must be set")
```

### ğŸ§ª Used in tests

```python
# conftest.py
secret = os.getenv("JWT_SECRET")
if not secret:
    raise RuntimeError("âŒ JWT_SECRET not set in environment")
```

âœ… `.env` is auto-loaded in local development and test runs.
âœ… Docker services consume `JWT_SECRET` via `docker-compose.yaml`.

> ğŸ” Before production use, replace with secure injection methods:
>
> * Docker secrets
> * CI/CD secret management
> * Vault-backed key providers

---

## ğŸ“Š Grafana Access

By default, the dashboard uses:

```env
GRAFANA_ADMIN_USER=admin         # âš ï¸ Used for initial dashboard provisioning and local testing only (ChangeMe)
GRAFANA_ADMIN_PASSWORD=llmops    # âš ï¸ Used for initial dashboard provisioning and local testing only (ChangeMe)
GRAFANA_ALLOW_ANON=true          # âš ï¸ Used for initial dashboard provisioning and local testing only (ChangeMe)
```

ğŸ“ Change these in `.env` for production deployments.

You can also enable or disable anonymous access via Grafana's `provisioning` config.

---

## ğŸ“Š Grafana Overview Dashboard

ğŸ“ `grafana/dashboards/llmops_overview.json` includes:

| Panel Title                 | Description                           |
| --------------------------- | ------------------------------------- |
| ğŸ“ˆ LLM Request Rate by User | Frequency of requests per unique user |
| ğŸ“‰ Latency by User (p95)    | p95 latency distribution by user ID   |

> âœ¨ Auto-loaded by Grafana on container start using provisioning config.
> âœ… Anonymous access enabled via `.env.example` credentials.

More panels (e.g., fallback %, token bar charts) can be added easily.

---

## ğŸ§ª Run Tests (Unit + E2E)

```bash
make test-unit     # Fast logic tests (auth, db, policy)
make test-e2e      # Full-stack smoke test w/ JWT and DB
```

> âœ… E2E tests simulate real API calls via HTTP, JWT, and DB assertions.

---

## âœ¨ Quickstart

```bash
git clone https://github.com/Cre4T3Tiv3/llmops-dashboard.git
cd llmops-dashboard
make init
```

This does everything:

* âœ… Verifies required tools (`docker`, `sqlite3`, `ollama`, etc.) via `make check`
* âœ… Installs [uv](https://github.com/astral-sh/uv) if missing
* âœ… Sets up `.venv` and installs `pyproject.toml` dependencies
* âœ… Auto-creates `.env` from `.env.example` if missing
* âœ… Confirms your selected Ollama model (e.g. `llama3`) is available locally

This project uses [`uv`](https://github.com/astral-sh/uv) â€” a **fast and modern Python package manager** â€” for all local and Docker-based dependency management.

> ğŸ“¦ No `requirements.txt` is needed â€” dependencies are resolved via `pyproject.toml`.

---

### âœ… Step 1: Verify Local Environment

Run the following to re-check your setup at any time:

```bash
make check
```

This confirms:

* âœ… Docker and docker-compose are available
* âœ… `sqlite3` is installed (required for `make smoke-test`)
* âœ… `.env` is present and contains necessary keys like `JWT_SECRET`
* âœ… `ollama` CLI is installed and working
* âœ… Your selected model (via `$OLLAMA_MODEL`) is installed

If the model is missing, youâ€™ll see a warning like:

```bash
âŒ Model 'llama3' not found in ollama list
```

> ğŸ’¡ This step is included in `make init` but can be run independently.

---

### ğŸ§ª Step 2: Launch the Full Stack

```bash
make up
```

This builds and starts:

| Service    | URL                                            |
| ---------- | ---------------------------------------------- |
| FastAPI    | [http://localhost:8000](http://localhost:8000) |
| Prometheus | [http://localhost:9090](http://localhost:9090) |
| Grafana    | [http://localhost:3000](http://localhost:3000) |

> âœ¨ Dashboard at Grafana auto-loads `grafana/dashboards/llmops_overview.json`

---

### ğŸ’¡ Want more? See:

ğŸ“„ [HOWTO and E2E Testing Guide](docs/HOWTO_and_E2E_Testing.md)
ğŸ¤ [Contributor Guide](docs/CONTRIBUTING.md)

---

## ğŸ§ª Sample Authenticated Request

```bash
make generate-jwt
curl -X POST http://localhost:8000/llm \
  -H "Authorization: Bearer <token>" \
  -H "x-user-id: demo-user" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What is RAG?"}'
```

---

## ğŸ“ Directory Layout
```text
llmops-dashboard/
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env
â”œâ”€â”€ .env.example
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .jwt.tmp
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.dev.md
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”œâ”€â”€ docker-compose.override.yml
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ CONTRIBUTING.md
â”‚   â””â”€â”€ HOWTO_and_E2E_Testing.md
â”œâ”€â”€ grafana/
â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â””â”€â”€ llmops_overview.json
â”‚   â””â”€â”€ provisioning/
â”‚       â””â”€â”€ dashboards/
â”‚           â””â”€â”€ dashboards.yaml
â”œâ”€â”€ llmops/
â”‚   â”œâ”€â”€ auth.py
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ mcp/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ client_tracker.py
â”‚   â”‚   â”œâ”€â”€ model_registry.py
â”‚   â”‚   â””â”€â”€ usage_policy.py
â”‚   â””â”€â”€ routes/
â”‚       â”œâ”€â”€ llm_echo.py
â”‚       â”œâ”€â”€ llm_proxy.py
â”‚       â””â”€â”€ token_issuer.py
â”œâ”€â”€ llmops_dashboard.egg-info/
â”œâ”€â”€ prometheus.yml
â”œâ”€â”€ pyproject.toml
â””â”€â”€ tests/
    â”œâ”€â”€ conftest.py
    â”œâ”€â”€ e2e/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ test_llm_echo.py
    â”‚   â”œâ”€â”€ test_llm_flow.py
    â”‚   â”œâ”€â”€ test_llm_traffic_simulation.py
    â”‚   â”œâ”€â”€ test_metrics_exposure.py
    â”‚   â””â”€â”€ test_smoke_flow.py
    â””â”€â”€ unit/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ test_database.py
        â”œâ”€â”€ test_mcp_policy.py
        â”œâ”€â”€ test_mcp_registry.py
        â”œâ”€â”€ test_mcp_tracker.py
        â””â”€â”€ test_reset_prometheus.py
```
---

## ğŸ› ï¸ Makefile Commands

```bash
make up                # Start full stack (FastAPI + Prometheus + Grafana)
make generate-jwt      # Create test JWT
make simulate          # Send mock traffic to /llm
make smoke-test        # Full E2E: token â†’ API â†’ DB â†’ metrics
make reset-prometheus  # Clean and rebuild metrics store
make clean             # Delete usage.db and logs
make nuke              # Destroy all containers, volumes, cache
```

> ğŸ’¡ All commands assume `uv` is installed locally. See [uv GitHub page](https://github.com/astral-sh/uv)


### ğŸ’¡ Want more? See:

ğŸ“„ [HOWTO and E2E Testing Guide](docs/HOWTO_and_E2E_Testing.md)
ğŸ¤ [Contributor Guide](docs/CONTRIBUTING.md)

---

## âš ï¸ Requirements

* Docker (v20+)
* Linux or WSL (native Windows not supported yet)
* Python â‰¥ 3.10 for CLI/test scripts (optional)
* [`uv`](https://github.com/astral-sh/uv) for local development and installs

---

## ğŸ“š Use Cases

* OpenAI/Ollama observability for internal tools
* Fine-grained request tracking (JWT, latency, token use)
* Test model fallback logic or simulate production LLM traffic
* Plug into billing or cost-monitoring with token metadata

---

## Built With

* [FastAPI](https://fastapi.tiangolo.com)
* [Prometheus](https://prometheus.io)
* [Grafana](https://grafana.com)
* [SQLite](https://www.sqlite.org/index.html)
* [Docker](https://www.docker.com)
* [uv](https://github.com/astral-sh/uv)

---

## ğŸ§  Philosophy

This project isnâ€™t just a toy â€” but itâ€™s also not a locked-in framework.

You can:

* Swap SQLite for Postgres
* Swap Prometheus for OpenTelemetry
* Swap FastAPI for Flask or Django
* Swap JWT with OAuth or session-based auth

The patterns are here.
The rest is yours to extend â™»ï¸

---

## ğŸ“œ License

MIT â€” see [`LICENSE`](./LICENSE)

---

> Built for the LLM observability era.
> OSS, modular, and easy to reason about ğŸ§ ğŸ“Š

---
