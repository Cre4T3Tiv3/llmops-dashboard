# ğŸ“Š LLMOps Dashboard

## âœ¨ Why This Exists

This is a **demo and prototype** of whatâ€™s possible to build yourself â€” using modern, open-source tools.

The goal is to **inspire LLM developers** to build their own observability layer using open standards and minimal infrastructure. Itâ€™s not a polished, production-ready framework â€” itâ€™s a realistic and extensible starting point for anything from GPT APIs to local Ollama pipelines.

This stack sets up a **containerized observability system** using:

**FastAPI + Prometheus + Grafana + SQLite**

Track request latency, token usage, fallback behavior, and per-user activity â€” all in real time, and in a form thatâ€™s easy to modify.

> ğŸ§ª Built for experimentation, learning, and extension â€” not production out-of-the-box.

* [ğŸ“˜ Setup & Troubleshooting Guide](HOWTO_and_TROUBLESHOOT.md)
* [ğŸ› ï¸ Developer Notes](README.dev.md)

> âš ï¸ **Note**: This project currently supports **Linux systems**.
> âœ… Works on **WSL** for Windows users.
> ğŸ§ª A native **Windows-compatible variant is planned**.

---

## ğŸ”§ What It Does (Current Showcase)

| Feature                 | Description                                                    |
| ----------------------- | -------------------------------------------------------------- |
| ğŸ” JWT-secured `/llm`   | Protects access and enables per-user tracking                  |
| ğŸ§  Fallback Simulation  | Randomized switching between `openai-gpt` and `local-ollama`   |
| ğŸ“Š Prometheus Metrics   | Captures request count, latency, token usage                   |
| ğŸ“ˆ Grafana Dashboards   | Visualize p95 latency + per-user traffic rates                 |
| ğŸ’¾ SQLite Audit Logs    | Stores prompt + metadata for inspection/audit                  |
| ğŸ§ª Simulation & Testing | `make simulate` and `make smoke-test` for traffic + validation |

---

## ğŸš€ How to Extend It

| Area        | Extension Ideas                                                                 |
| ----------- | ------------------------------------------------------------------------------- |
| ğŸ”§ Models   | Replace simulation with real OpenAI or Ollama inference                         |
| ğŸ›¡ï¸ Auth    | Plug in real token issuance and validation logic                                |
| ğŸ“¡ Webhooks | Forward logs to Loki, Elasticsearch, or alerting systems                        |
| ğŸ“‚ Uploads  | Accept documents to simulate chunking / RAG pipelines                           |
| ğŸ’µ Billing  | Add token-based cost tracking via `tiktoken` or OpenAI usage data               |
| ğŸ”” Alerts   | Use Prometheus alert rules for spikes, fallback behavior, or latency thresholds |

---

## ğŸ“Š Current Grafana Dashboard (Simplified + Working)

ğŸ“ `grafana/dashboards/llmops_overview.json` currently includes:

| Panel Title                 | Description                                  |
| --------------------------- | -------------------------------------------- |
| ğŸ“ˆ LLM Request Rate by User | Visualizes request frequency per user        |
| ğŸ“‰ Latency by User (p95)    | Shows latency distribution (95th percentile) |

> âœ… Focused and stable. Additional panels (stat counters, fallback %) can be added later.

---

## ğŸ§  System Architecture

```mermaid
graph TD
  Client[ğŸ§‘ Client]
  Token[ğŸ” /auth/token]
  LLMAPI[âš™ï¸ /llm Endpoint]
  JWT[ğŸ”‘ JWT Middleware]
  DB[(ğŸ—„ï¸ SQLite)]
  Prometheus[ğŸ“ˆ Prometheus]
  Grafana[ğŸ“Š Grafana]
  OpenAI[(â˜ï¸ Simulated LLM)]

  Client --> Token
  Client -->|JWT| LLMAPI
  LLMAPI --> JWT
  LLMAPI --> DB
  LLMAPI --> Prometheus
  LLMAPI --> OpenAI
  Prometheus --> Grafana
```

---

## ğŸš€ Quickstart

```bash
git clone https://github.com/Cre4T3Tiv3/llmops-dashboard.git
cd llmops-dashboard
make up
```

Access:

* FastAPI: [http://localhost:8000](http://localhost:8000)
* Metrics: [http://localhost:8000/metrics](http://localhost:8000/metrics)
* Grafana: [http://localhost:3000](http://localhost:3000)
  Login: `admin / admin`

---

## ğŸ§ª Sample Request

```bash
make generate-jwt  # get token
curl -X POST http://localhost:8000/llm \
 -H "Authorization: Bearer <your-jwt-token>" \
 -H "Content-Type: application/json" \
 -H "x-user-id: demo-user" \
 -d '{"prompt": "What is vector search?"}'
```

---

## ğŸ“ Directory Layout

```text
llmops-dashboard/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ auth.py
â”‚   â”œâ”€â”€ database.py
â”‚   â””â”€â”€ routes/
â”‚       â”œâ”€â”€ llm_proxy.py
â”‚       â””â”€â”€ token_issuer.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ simulate_llm_traffic.sh
â”‚   â”œâ”€â”€ smoke_test.sh
â”‚   â””â”€â”€ reset_prometheus_data.sh
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â””â”€â”€ prometheus.yml
â”œâ”€â”€ grafana/
â”‚   â””â”€â”€ dashboards/
â”‚       â””â”€â”€ llmops_overview.json
â”œâ”€â”€ data/
â”‚   â””â”€â”€ usage.db  # (created automatically)
â”œâ”€â”€ .env.example
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ README.dev.md
â””â”€â”€ HOWTO_and_TROUBLESHOOT.md
```

---

## ğŸ› ï¸ Helpful Commands

```bash
make up              # Start the full stack
make down            # Stop and clean containers
make logs            # View FastAPI logs
make shell           # Enter FastAPI container shell
make simulate        # Send demo traffic to /llm
make smoke-test      # Validate all endpoints
make generate-jwt    # Create a demo JWT for /llm requests
make reset-prometheus # Wipe Prometheus metrics history
```

---

## ğŸ› ï¸ Built With

* [FastAPI](https://fastapi.tiangolo.com)
* [Prometheus](https://prometheus.io)
* [Grafana](https://grafana.com)
* [SQLite](https://www.sqlite.org/index.html)
* [Docker](https://www.docker.com)

---

## ğŸ“š Use Cases

* Secure OpenAI request logging and analytics
* Debug model fallback logic and simulate reliability under load
* Track per-user latency and request volume
* Prototype real LLM pipelines with structured observability

---

## ğŸ“œ License

MIT â€” see `LICENSE`

---